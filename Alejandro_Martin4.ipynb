{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teEa-sY96F9U"
      },
      "source": [
        "# Notebook 04 – Comparación de Modelos y Conclusiones\n",
        "\n",
        "En este notebook se presentan las métricas obtenidas por todos los modelos entrenados en los notebooks anteriores:\n",
        "- **Notebook 03**: Logistic Regression y Linear SVM (modelos tradicionales)\n",
        "- **Notebook 05**: LSTM con Word2Vec (modelo de Deep Learning)\n",
        "\n",
        "Se realiza un análisis comparativo completo y se elige el modelo final para el proyecto de análisis de sentimiento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtFA-pZ6JIn"
      },
      "source": [
        "## Resumen de Métricas por Modelo\n",
        "\n",
        "### Modelos Tradicionales (Notebook 03)\n",
        "\n",
        "**Logistic Regression:**\n",
        "- Accuracy: 0.839\n",
        "- Precision (clase 0): 0.83, Recall: 0.86, F1-score: 0.84\n",
        "- Precision (clase 1): 0.85, Recall: 0.82, F1-score: 0.84\n",
        "- F1-score promedio: 0.84\n",
        "\n",
        "**Linear SVM:**\n",
        "- Accuracy: 0.82\n",
        "- Precision (clase 0): 0.81, Recall: 0.83, F1-score: 0.82\n",
        "- Precision (clase 1): 0.83, Recall: 0.81, F1-score: 0.82\n",
        "- F1-score promedio: 0.82\n",
        "\n",
        "### Modelo Deep Learning (Notebook 05)\n",
        "\n",
        "**LSTM con Word2Vec:**\n",
        "- Accuracy: 0.716\n",
        "- Precision (clase 0): 0.68, Recall: 0.82, F1-score: 0.74\n",
        "- Precision (clase 1): 0.77, Recall: 0.61, F1-score: 0.68\n",
        "- F1-score promedio: 0.71\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t91XvzM6LKK"
      },
      "source": [
        "## Análisis Comparativo\n",
        "\n",
        "### Comparación de Accuracy\n",
        "\n",
        "1. **Logistic Regression**: 0.839 (mejor)\n",
        "2. **Linear SVM**: 0.82\n",
        "3. **LSTM con Word2Vec**: 0.716\n",
        "\n",
        "### Comparación de F1-score Promedio\n",
        "\n",
        "1. **Logistic Regression**: 0.84 (mejor)\n",
        "2. **Linear SVM**: 0.82\n",
        "3. **LSTM con Word2Vec**: 0.71\n",
        "\n",
        "### Observaciones\n",
        "\n",
        "- **Modelos tradicionales**: Los modelos basados en CountVectorizer y regresión logística/SVM obtienen mejores resultados que el modelo de Deep Learning en este caso. Esto puede deberse a:\n",
        "  - El tamaño del dataset (5000 muestras) puede ser insuficiente para aprovechar completamente las capacidades de una red neuronal\n",
        "  - Los modelos tradicionales con n-gramas capturan bien los patrones de sentimiento en reviews cortas\n",
        "  - El preprocesado con stopwords y stemming funciona bien con modelos tradicionales\n",
        "\n",
        "- **LSTM con Word2Vec**: Aunque obtiene menor accuracy, tiene mejor recall en la clase negativa (0.82 vs 0.86 y 0.83), lo que significa que detecta mejor las reviews negativas. Sin embargo, tiene problemas con la clase positiva (recall 0.61), lo que sugiere que el modelo podría necesitar más ajuste de hiperparámetros o más datos de entrenamiento.\n",
        "\n",
        "- **Equilibrio de clases**: Logistic Regression muestra el mejor equilibrio entre precisión y recall en ambas clases, lo que indica una mejor capacidad de generalización.\n",
        "\n",
        "## Modelo Final Elegido\n",
        "\n",
        "El modelo final elegido es **Logistic Regression**.\n",
        "\n",
        "Este modelo muestra:\n",
        "- El mejor accuracy global (0.839)\n",
        "- El mejor F1-score promedio (0.84)\n",
        "- Mejor equilibrio entre precisión y cobertura en ambas clases\n",
        "- Menor número de errores totales en la matriz de confusión\n",
        "- Mayor estabilidad y consistencia en el rendimiento\n",
        "\n",
        "Aunque el modelo LSTM tiene potencial para mejorar con más datos y ajuste de hiperparámetros, para este proyecto y este tamaño de dataset, Logistic Regression ofrece el mejor rendimiento con menor complejidad computacional.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualización Comparativa\n",
        "\n",
        "Creo una visualización para comparar las métricas de los tres modelos de manera gráfica y facilitar la interpretación de los resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Datos de los modelos\n",
        "modelos = ['Logistic\\nRegression', 'Linear SVM', 'LSTM\\nWord2Vec']\n",
        "accuracy = [0.839, 0.82, 0.716]\n",
        "f1_score = [0.84, 0.82, 0.71]\n",
        "\n",
        "# Crear gráfico comparativo\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Gráfico de Accuracy\n",
        "ax1.bar(modelos, accuracy, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Comparación de Accuracy')\n",
        "ax1.set_ylim([0.6, 0.9])\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(accuracy):\n",
        "    ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Gráfico de F1-score\n",
        "ax2.bar(modelos, f1_score, color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "ax2.set_ylabel('F1-Score Promedio')\n",
        "ax2.set_title('Comparación de F1-Score')\n",
        "ax2.set_ylim([0.6, 0.9])\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(f1_score):\n",
        "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtO1hWkx6O_G"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
